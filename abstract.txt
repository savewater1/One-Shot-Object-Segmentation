Semantic segmentation is a classic vision problem which has many applications which include, but is not limited to robotics, autonomous driving, virtual reality, etc. Unlike detection, segmentation provides a more precise object boundary which is especially useful in the field of robotics. Semantic segmentation is a highly researched topic, and most of the proposed methods require a large amount of data to train accurate models. While getting image data is not very difficult because of the internet, getting a large amount of annotated data requires a lot of resources. Therefore, in this project we are focusing on the problem of one-shot object segmentation, i.e. Segmenting an object using only one annotated example per class. We implemented approaches from two papers, compared and evaluated them on RGB-D Object Dataset, and improved the results by incorporating depth information using MRF. We also discuss the limitations of these models as seen on this dataset.